{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a613ba7a-ad7d-4452-8717-8153643d575d",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55541a77-baf5-4fa4-870e-97f00086edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "class CustomOllama(Ollama):\n",
    "    def __init__(self, model, callback_manager=None, stop=None, temperature=0.2):\n",
    "        super().__init__(model=model, stop=stop, temperature=temperature)\n",
    "        self.callback_manager = callback_manager\n",
    "\n",
    "# Initialize the CallbackManager\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Create an instance of the CustomOllama class\n",
    "llm = CustomOllama(\n",
    "    model=\"llama3\",\n",
    "    callback_manager=callback_manager,\n",
    "    stop=[\"\"]\n",
    ")\n",
    "\n",
    "temperature = 0.2\n",
    "#higher temperature = more creativity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7bbeb-55b7-4c46-8fac-c92c47714391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test \n",
    "prompt = [\"Why is the sky blue?\"]  # Prompt should be a list of strings\n",
    "\n",
    "# Generate text using the Ollama model\n",
    "generated_text = llm.generate(prompts=prompt, temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f748b9-0393-4ec8-b366-a19d4e4cc35d",
   "metadata": {},
   "source": [
    "## Setup RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6111bb2-f2bc-4825-bbdf-7f1f2183d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a3604-4180-4c27-a77f-ceedf51f0ae7",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846fd790-b1d1-4d0f-bab9-0eb311cee5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You're a helpful AI assistant for a jobseeker with a criminal conviction trying to understand what legal restrictions exist for jobs and certifications.\"\n",
    "    \"Given a user question and sections of Virginia law code, answer the user question. If none of the sections of the law code from Virginia answer the question\"\n",
    "    \"Just say you don't know.\"\n",
    "    \"\\n\\nUse only the following sections of the law code to answer the question. Provide the source from the context: \"\n",
    "    \"{context}\"\n",
    "    )\n",
    "\n",
    "prompt_plain = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "prompt_plain.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845879c-6d08-4a5f-bcfd-3d4bbf2bdeda",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa8d58-5baf-477d-8381-325bd04a3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableMap\n",
    "\n",
    "# Function to format documents with their metadata\n",
    "def format_docs(docs: List[Document]):\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        metadata_str = \"\\n\".join(f\"{key}: {value}\" for key, value in doc.metadata.items())\n",
    "        formatted_doc = f\"Metadata:\\n{metadata_str}\\n\\nContent:\\n{doc.page_content}\"\n",
    "        formatted_docs.append(formatted_doc)\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Chain to format documents, process with LLM, and parse the output\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt_plain  # Assuming 'prompt' is defined somewhere in your setup\n",
    "    | llm  # Assuming 'llm' is defined somewhere in your setup\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Function to retrieve documents based on a query\n",
    "retrieve_docs = (lambda x: x[\"input\"]) | retriever  # Assuming 'retriever' is defined somewhere in your setup\n",
    "\n",
    "# Define the final chain\n",
    "chain_plain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc9acce-4e20-4f05-8748-f8640197afc7",
   "metadata": {},
   "source": [
    "### Test with prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915069f7-955e-471c-927b-febe087b4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a string for your prompt template\n",
    "query = \"What job restrictions exist for someone convicted of a crime?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad1a6f-7337-4ee0-8cc0-f10617c83eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain_plain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526550af-1c8a-412d-8e40-5228fbb35174",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['context']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba165e7-8419-4a50-83f7-6bbd754af825",
   "metadata": {},
   "source": [
    "# Trulens Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76780814-bcd5-44d9-abe2-f906f8cf0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local IP address command \n",
    "# !curl ifconfig.me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e46298-c91d-42b9-bc6e-b77881cfa141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trulens_eval\n",
    "# Imports main tools:\n",
    "from trulens_eval import TruChain, Tru\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d0018-440c-4e13-90d5-282dcc1f71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LiteLLM-based feedback function collection class:\n",
    "from langchain.llms import Ollama\n",
    "from trulens_eval import LiteLLM\n",
    "import litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087dfba-a520-49c3-afea-ebda5b3f925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'http://localhost:11435' refers to everyone's local device\n",
    "ollama_provider = LiteLLM(model_engine='ollama/llama3', api_base='http://174.20.175.133:11435')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e861c9-7258-4228-82d6-e8d40519149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens_eval import Feedback, Select\n",
    "\n",
    "context = result['context']\n",
    "context_texts = [doc.page_content for doc in context]\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(ollama_provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
    "    .on(Select.RecordCalls.args.context)  # This selects the context from the function call\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer\n",
    "f_answer_relevance = (\n",
    "    Feedback(ollama_provider.relevance_with_cot_reasons, name=\"Answer Relevance\")\n",
    "    .on(Select.RecordCalls.args.query)  # This selects the query/input\n",
    "    .on_output()  # This selects the output directly\n",
    ")\n",
    "\n",
    "# Context relevance between question and each context chunk\n",
    "f_context_relevance = (\n",
    "    Feedback(ollama_provider.context_relevance_with_cot_reasons, name=\"Context Relevance\")\n",
    "    .on(Select.RecordCalls.args.query)  # This selects the query/input\n",
    "    .on(Select.RecordCalls.args.context)  # This selects the context\n",
    "    .aggregate(np.mean)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb9a6a-43b3-4e11-b1a7-1d4ffaba86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruChain(\n",
    "    app=llm,\n",
    "    app_id='Chain1_ChatApplication',\n",
    "    feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness], \n",
    "    selectors_check_warning=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c760809-b8de-457e-86fa-2075406fa6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    llm_response = chain_plain.invoke({\"input\": query})\n",
    "\n",
    "display(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
